---
title: "NIVEL FLU TREND"
output: html_notebook
---
*Schneider PP^1^,Paget J^2^,  Donker G^2^, Hooiveld M^2^, Spreeuwenberg P^2^, Donker G^2^, van Gool C^3^*
*^1^ Maastricht University, Netherlands Institute of Health Service Research*
*^2^ Netherlands Institute of Health Service Research*
*^3^ Maastricht University, Netherlands Institute of Health Service Research*
*Contact*

# Introduction

# NIVEL Flu Trend Algorithm



## Load the required packages
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
# All the required packages
required_packages<-c("RCurl","ISOweek","jsonlite",
                     "ggplot2","prophet","dplyr",
                     "wikipediatrend","pageviews","caret",
                     "imputeTS","gridExtra",
                     "cowplot","doParallel","glmnet",
                     "Cubist","pls","devtools",
                     "sandwich","elasticnet","plyr") 
# Function to install (if needed) and load the required packages
pft_packages <- function(package){
  for(i in 1:length(package)){
    if(eval(parse(text=paste("require(",package[i],")")))==0) {
      install.packages(package)
      }
    }
  return (eval(parse(text=paste("require(",package,")"))))
  }
    
    pft_packages(required_packages)
    
    # The newest developer version of gtrendsR is required:
    # devtools::install_github('PMassicotte/gtrendsR')  
    library(gtrendsR)
```
    

## 2 .Set parameters

We need to specify the following parameters:

  * `to`          = Retrieving data up until this date. Maximum: Yesterday
  * `from`        = Starting data retrival at* 
  * `forecast.to` = Forecasting until this date 
  * `term`        = Start algorithm with an existing Wikipedia Article on Influenza
  * `country_of_interest` = Country of interest in ISO_3166-2  
  * `language_of_interest` = Language of interest in ISO_639-1 
  * `incidence.data.path` = File path where the outcome data can be found
  * `show.me.plots` = Optional: Do you want to run the code step-by-step and inspect plots?
  * `status`       = Optional: Show notifications?
  
* Depends on available outcome data. If a peirod of > 5 years is selected Google Trends might create some issues, because it only delivers monthly, inseatd of weekly data...

```{r, set_parameters}
    to = as.Date(Sys.Date())-2
    from = min(seq(from=(as.Date(Sys.Date())-2),  # makes sure we use the right date
                   length.out = 52.14286*5,       
                   by=-7)) 
    forecast.to = Sys.Date()-1 + 35 
    term = "Griep" 
    country_of_interest = "NL"
    language_of_interest = "nl"
    incidence.data.path = "ili2.csv" 
    show.me.plots = 1
    status = 1
    time1 = Sys.time() # tracks running time
```


## 3. NIVEL Flu Trend Functions

List of functions:

  * `fGetWikipediaData` = Download data from Wikipedia
    * pages: Name(s) of article(s) (e.g. "Griep")
    * language_of_interest =  language in ISO_639-1  (e.g. "nl")
    * from: Date e.g. as.Date("2010-08-01")
    * to:  Date e.g. as.Date("2016-07-31")
    * status: show notification (e.g. 1)
    
  * `fWiki_lp`          = Get the Wikipedia articles with links on the page of interest
    * term: Name(s) of article(s) (e.g. "Griep")
    * language_of_interest:  language in ISO_639-1  (e.g. "nl")
    * backlinked : Also get pages that link 'to' the page of interest (1 or 0)
    * manual.pages: manually add terms to the resulting list (e.g. "Koud")
    
  * `fGetGoogleData`    = Get related keywords and download data from Google Trends
    * keyword: Name(s) of keywords(s) (e.g. "Griep") 
    * country_of_interest: country in ISO_3166-2  (e.g. "NL")
    * from: Date e.g. "2015-08-01",
    * to: Date e.g. "2017-07-31",
    * status: Show notifications, 1 or 0
    * prefix: Add identifier to column names (e.g. "g.trends.")
    * gprop = Google Trends gprop ("web" or "news")
    
  * `fRescaleGtrends`   = Rescales overlapping google data
    * df.1 and df.2 are two overlapping dataframe with vectors week and hits
  * `fEvalModel`        = Evaluate models created with the `caret` package
    * Takes as input a model created with `train` from `caret`
    * Provides plots and statistics
  
```{r}
# Retrieving Wikipedia page view data
# Function uses Wikipedia Pageview API and the Wikishark Website
fGetWikipediaData = function(pages = wiki.pages[1:3],
                             language_of_interest =  "nl", 
                             from = as.Date("2010-08-01"),
                             to = as.Date("2016-07-31"),
                             status=1){    # Print status whily running?
      
      # Wikishark needs this format
      from.wikishark = format(from,format="%m/%d/%Y")  
      to.wikishark = format(as.Date("2015-09-30"),format="%m/%d/%Y")
      
      # Wikipedia API needs this format
      from.wikiapi = as.Date("2015-08-01") 
      to.wikiapi = as.Date(to)
      
      if(to == Sys.Date()){to = as.Date(to)-1} # Wikipedia API can only provide data up until yesterday
      
      wiki = data.frame(date=seq(from=as.Date(from),to=as.Date(to),by=1)) # Reference dataframe
      for(p in 1:length(pages)){   # loop for downloadind the pages
        tryCatch({
          
          if(status==1){ # print status?
            cat("Downloading data for page",p,"of",length(pages)," - ",(p/length(pages))*100 ,"% \n")
          }
          
          if(to >= as.Date("2015-08-01")){  # For wikipedia data > this date, we use Wikipedia API
            temp.dat<-article_pageviews(article = pages[p],   # FUN from the 'pageviews'-package
                                        project = paste(language_of_interest,".wikipedia",sep=""),
                                        start = "2015080100",end = pageview_timestamps(to))
            temp.wiki.pageview = data.frame(as.Date(temp.dat$date),(temp.dat$views)) 
            names(temp.wiki.pageview) = c("date",pages[p])
          }
          
          if(from < as.Date("2015-08-01")){ 
            # For data < this date, we retrieve the messy data from Wikishark
            #  First, identify the wikishark page ID which corresponds to the wikipedia article

            url.lookup = paste("www.wikishark.com/title/",language_of_interest,"/",pages[p],sep="")
            raw.url.lookup = RCurl::getURLContent(url.lookup)
            # identify the ID between these two expressions:
            start.at =regexpr("translate/id/",raw.url.lookup)[1] 
            stop.at = regexpr("values=",raw.url.lookup)[1]
            page.id =  substr(raw.url.lookup,start.at+nchar("translate/id/"),stop.at-2)
            # Look at the Wikishark page that shows the data for the Wikipedia article
            ws.url = paste("http://www.wikishark.com/json_print.php?values=",
                           page.id,"&datefrom=",from.wikishark,"&dateto=",
                           to.wikishark,"&view=2&normalized=0&scale=0&peak=0&log",
                           "=0&zerofix=0&sumall=0&format=csv", sep="")
            # Download the data from the website
            ws.csv = read.csv(ws.url)[,1:2]
            temp.ws.pageview = data.frame(as.Date( ws.csv[,1]
                                                   ,format="%m/%d/%Y"),
                                          (as.numeric(ws.csv[,2])))
            names(temp.ws.pageview) = c("date",pages[p])
            
            # Row-bind wikishark and wikipedia api data (if neccessary)
            if(to >= as.Date("2015-08-01")){
              temp.wiki.pageview = rbind(temp.wiki.pageview,temp.ws.pageview)
              } else {
              temp.wiki.pageview = temp.ws.pageview
            }
            
          }
          
          # Merge the data of different wikipedia
          wiki = merge(wiki,temp.wiki.pageview,by="date",all=T) 
        },
        # if data can not retrieved from one of the sources, the page will be dropped entirely
        error=function(e) {if(status==1){cat("Uups...Something went wrong with",pages[p],"\n")}})
      }
      # Add .wiki to columns, so preditors can be attributed to wikipedia later on 
      names(wiki)[-1] = paste("wiki.",names(wiki)[-1],sep="") 
      
      # Aggregate dail data to weekly data
      aggregate.data = aggregate(wiki[,-1],
                                 list(ISOweek(wiki$date)),
                                 FUN=function(x){mean(x,na.rm=T)},simplify=T)[,2]
      wiki= data.frame(date=unique(ISOweek(wiki$date)),
                       aggregate.data)
      return(wiki)
    }
    

# Retrieve linked and backlinked pages
# Function looks up all the internal links to other Wikipedia pages
fWiki_lp = function(term = "Influenza",
                       language_of_interest = "de", 
                       backlinked = 1 ,  # Also get pages that link to the page of interest
                       manual.pages=c("Halsschmerzen")){ # Manually add terms to the resulting list or "F"
  
      # Query the Wikipedia API
      wiki.query<-paste("https://",language_of_interest, 
                        ".wikipedia.org/w/api.php?action=query&prop=links&pllimit=",
                        "500&format=json&titles=",term, sep="")
      linked_pages<-fromJSON(getURL(wiki.query)) 
      links = linked_pages$query$pages[[1]]$links$title
        
      # If the number of linked pages is larger than 500 
      # (on the English Influenza page there are 660 links), 
      # we need to send multiple queries using the code below
      if( !is.null(linked_pages$continue[1])){
        temp.links<-NULL
        while(!is.null(linked_pages$continue[1])){ 
          # Only 500 linked pages can be retrieved per query, if >500, more than 1 query is neccessary
          linked_pages<-paste("https://",language_of_interest,
                          ".wikipedia.org/w/api.php?action=query&prop=links&pllimit",
                          "=500&format=json&titles=",term,"&plcontinue=",
                          linked_pages$continue[1],  sep="")
          linked_pages<-fromJSON(getURL(linked_pages))
          temp.links<-linked_pages$query$pages[[1]]$links$title
          }
          links = c(links,temp.links) # combine rounds of 500+ queries
          }
    
      
      # Query for backlinks: Pages that link to the page of interest
      #      CAVE: ONLY THE FIRST 500 LINKS ARE RETREIEVED.
      if(backlinked ==1 ){
        backlinked.pages<-paste("https://",language_of_interest,
                                ".wikipedia.org/w/api.php?action=query&list=backlinks&bllimit",
                                "=500&format=json&bltitle=",term,  sep="")
        backlinked_pages<-fromJSON(getURL(backlinked.pages)) 
        backlinks = NULL
          backlinks<-backlinked_pages$query$backlinks$title
      } else {
            backlinks = NULL
            }
    
      if(manual.pages == F | manual.pages == ""){
        manual.pages = NULL
      }
      
      # Combine all retrieved wikipedia article names
      wiki.pages = c(term,links,backlinks,manual.pages)
      wiki.pages = gsub(" ","_",wiki.pages) # Substitutes spaces with "_"
      wiki.pages = unique(wiki.pages) # Remove duplicated terms
      return(wiki.pages)
    }
    

# A function to rescale overlapping data points from Google Trends data
fRescaleGtrends = function(df.t1,df.t2) { 
            # Takes two data frames with a numeric vector and rescales them...
            # Data from Google trends from different time spans does not match 100%
            # (Because it is a sample of the actual data and measures 'relative interest')
            # Therefore, it needs to be re-scaled
            # We use a simple linear regression....
            match1 = df.t1[match(df.t2$week,df.t1$week),]
            match1 = match1[!is.na(match1$week),]
            match2 = df.t2[match(df.t1$week,df.t2$week),]
            match2 = match2[!is.na(match2$week),]
            rescale.factor = lm(data=match1,match2$hits ~ hits+0) 
            df.t1$hits = round(predict(rescale.factor,newdata = df.t1),1) 
            df.t1=df.t1[df.t1$week<min(df.t2$week),]
            hits = as.numeric(c(df.t1$hits,df.t2$hits))
            weeks = c(df.t1$week, df.t2$week)
            scaled.df = data.frame(weeks,hits,stringsAsFactors = F)
            return(scaled.df)
            }

    
# GET GOOGLE DATA
fGetGoogleData = function(keyword = "influenza",
                          country_of_interest="DE",
                          from="2015-08-01",
                          to="2017-07-31",
                          status= 1,
                          prefix="g.trends.",
                          gprop = "web") {
      
      # CAVE: 
      # Becaue the level of aggregation and the latest available data 
      # differs, depending on the time range we specifiy, we usually have to run multiple
      # queries to get what we want...
      # This means, the function runs 4(!!!) queries per keyword. (Could be optimized?!)
      # After an unknown amount of queries, Google blocks the API
      # Then you might get the error "res$status_code == 200 is not True..."
      # Try on the next day, or use another IP/Session
  
      # Set time spans
      time.span1 =  paste(as.Date(from), as.Date(to))
      time.span1.dates = ISOweek(seq(from=as.Date(from),to=as.Date(to), by=7))
      time.span = time.span1.dates
      time.span.5y = time.span1
      time.span.3m = "today 3-m"
      time.span.1m = "today 1-m"
      time.span.1w = "now 7-d"
      
      # Reference Matrix
      google.input.data = as.data.frame(matrix(nrow = length(time.span),
                                               ncol = 1+length(keyword)))
      google.input.data[,1] =  time.span # reference dates
      
      # Loop for downloading the search query data
      for(p in 1:length(keyword)){ 
        if(status==1){ # print status
          cat("asking Google for statistics for",
              keyword[p]," - ",round(p/length(keyword),3)*100,"%" ,"\n")
          }
        tryCatch({ 
          # download data for a 5 year range (missing the most recent weeks)
          google.temp.5y = gtrends(keyword = keyword[p],
                                   geo = country_of_interest,
                                   time= time.span.5y ,gprop = gprop) 
          google.temp.5y = google.temp.5y$interest_over_time[,c(1,2)]
          google.temp.5y$week = ISOweek(google.temp.5y$date)
          
          Sys.sleep(0.1) # pause to avoid being blocked- not sure what Google's rules are
          # download data for the last 3 months, missing most recent days
          google.temp.3m = gtrends(keyword = keyword[p],
                                   geo = country_of_interest,
                                   time= time.span.3m ,gprop = gprop) 
          google.temp.3m = google.temp.3m$interest_over_time[,c(1,2)]
          google.temp.3m$week = ISOweek(google.temp.3m$date)
          
          # Can't remeber what this is helping with:
          if(sum(as.Date(google.temp.3m$date)>to)>0){
            google.temp.3m = google.temp.3m[-which(as.Date(google.temp.3m$date)>to),]}
          google.temp.3m = aggregate(hits ~ week, google.temp.3m, mean)
          
          Sys.sleep(0.1) # breath
          # get more recent data
          google.temp.1m = gtrends(keyword = keyword[p],
                                   geo = country_of_interest,
                                   time= time.span.1m ,gprop = gprop)
          google.temp.1m = google.temp.1m$interest_over_time[,c(1,2)]
          google.temp.1m$week = ISOweek(google.temp.1m$date)
          if(sum(as.Date(google.temp.1m$date)>to)>0){
            google.temp.1m = google.temp.1m[-which(as.Date(google.temp.1m$date)>to),]}
          google.temp.1m = aggregate(hits ~ week, google.temp.1m, mean)
          
          Sys.sleep(0.1) 
          # get most recent data
          google.temp.1w = gtrends(keyword = keyword[p],
                                   geo = country_of_interest,
                                   time= time.span.1w ,gprop = gprop)
          google.temp.1w = google.temp.1w$interest_over_time[,c(1,2)]
          google.temp.1w$week = ISOweek(google.temp.1w$date)
          if(sum(as.Date(google.temp.1w$date)>to)>0){
            google.temp.1w = google.temp.1w[-which(as.Date(google.temp.1w$date)>to),]
            }
          # This is daily data, so it needs to be aggregated to weeks
          google.temp.1w = aggregate(hits ~ week, google.temp.1w, mean)
          
          
          # rescale all the different time periods to each other
          scaled.df.5y.3m = fRescaleGtrends(google.temp.5y,google.temp.3m)
          scaled.df.5y.3m.1m = fRescaleGtrends(scaled.df.5y.3m,google.temp.1m)
          scaled.df.5y.3m.1m.1w = fRescaleGtrends(scaled.df.5y.3m.1m,google.temp.1w)
          
          # insert data into the reference matrix
          google.input.data[,p+1] = as.numeric(scaled.df.5y.3m.1m.1w$hits)
          },  error=function(e) { 
          if(status==1){ 
            cat("\n Uups...Something went wrong with",keyword[p],"\n")
          }
            Sys.sleep(0.2) # breath again...
        })
        
      }
      # give the columns a prefix to identify them later on
      names(google.input.data) = c("date",paste(prefix,gsub(" ","\\.",keyword),sep=""))
      return(google.input.data)
    }
    
    
    
    
## eval functions for models
fEvalModel = function(model){ 
    # Function to get some plots and statistics for the prediction models
    # depends on data sets like df.train, df.test, date.train etc that will be created later on
    
      cv.plot = NULL
      pred.plot = NULL
      cor.train = NULL
      cor.test = NULL
      cv.Rsquared = NULL
      lowest.rmse = NULL
      
      # CV Plots
      tryCatch({  # For some models, there are no tuning parameters
        if(!exists("parameter",where=model$bestTune)){
          
          cv.plot = plot(model)}
      },
      error=function(e) {})
      
      # Predictions, RMSE, prediction plots
      if(!is.null(model)){
        tryCatch({
          # Predcitions
          pred.train.temp = predict(model,newdata= df.train)
          rmse.train = round(sqrt(mean( (pred.train.temp - y.train)^2 )),6)
          pred.test.temp =  predict(model,newdata= df.test)
          # combine into training and test data sets+outcome+prediction
          train.temp.df  = data.frame(y=y.train,date=date.train,preds=pred.train.temp)
          test.temp.df = data.frame(date=date.test,preds=pred.test.temp)
          
          model.name.for.title = ifelse(exists("name.of.model"),name.of.model,model$method)
          
          # Plot y versus prediction for training and test data
          pred.plot = ggplot(data=train.temp.df) +
            geom_line(data=train.temp.df,aes(x=date.train ,y=y.train),col="black") +
            geom_point(data=train.temp.df,aes(x=date.train,y=pred.train.temp),col="orange") +
            geom_line(data=train.temp.df,aes(x=date.train,y=pred.train.temp),col="orange") +
            geom_point(data=test.temp.df,aes(x=date.test,y=pred.test.temp),col="red")  +
            geom_line(data=test.temp.df,aes(x=date.test,y=pred.test.temp),col="red") +
            ggtitle(paste(model.name.for.title)) +
            ylab("y - actual vs. predicted") +
            xlab("Date - Training and Test period") +
            theme_light()
          
        },error = function(e){cat(": Error \n")})
        
      }
      
      # Get Correlation between y and predictions and R-squares 
      tryCatch({
        cor.train =  cor(y.train,pred.train.temp)
        cv.Rsquared = model$results$Rsquared
        lowest.rmse =  min(model$results$RMSE,na.rm=T)
      },error = function(e){cat(": Error \n")})
      
      # Output a list
      out = list(plots=list("cv.plot"=cv.plot,
                            "pred.plot" =pred.plot),
                 correlations = list("cor.train" = cor.train,
                                     "cor.test" = cor.test,
                                     "cv.Rsquared" = cv.Rsquared),
                 "lowest.rmse" = lowest.rmse)
      return(out)
    }
```


## 4. Outcome data

```{r}    
# Load ili outcome data
if(status==1){ cat(" \n Loading ili data") }
ili.nl.data = read.csv(incidence.data.path)
# The rest of this code is used to deal with the particularities of this file
names(ili.nl.data) = c("date","y") 
ili.nl.data$date = as.Date(ili.nl.data$date)
ili.nl.data = ili.nl.data[ili.nl.data$date>= from,]
# show an example of the ili- data
if(show.me.plots==1) {head(ili.nl.data)}
# split.at defines the date which separates train and test (i.e.nowcast) data
# depends on your ili data and when you want to run the model 
# e.g. -1, -2, -x 
split.at = max(ili.nl.data$date) - 1 
    
outcome.ili.plot = ggplot(ili.nl.data) +
  geom_line(aes(x=date,y=y)) + 
  ggtitle("Outcome ili data")
    
if(show.me.plots==1) {outcome.ili.plot}
```



## 5. Get Data from Wikipedia and Google

```{r}
# Identify wiki articles linked to the 'griep' article
    if(status==1){ cat("\n Retrieving Wikipedia page info")}

wiki.pages = fWiki_lp(
  term = term,                    
  language_of_interest = "nl",     
  backlinked = 0,                  
  manual.pages=c(""))              
    
    
# Download the Wikipedia data from Wikishark and Wikipedia API 
    if(status==1){ cat("\n Downloading Wikipedia data")}
  # Create a reference data frame
wiki.df = 
  fGetWikipediaData(pages = wiki.pages[1],        
                    language_of_interest =  "nl", 
                    from = from,    
                    to = to,    
                    status = 0)  
    wiki.df  = data.frame(date=wiki.df$date)
# Loop over all the selected wikipedia articles
for(i in 1:length(wiki.pages)){
  if(status==1){ cat(i,"of",length(wiki.pages), " ",round(i/length(wiki.pages),4)*100,"% \n")}
  tryCatch({
    nl.wikipedia.input.data = fGetWikipediaData(pages = wiki.pages[i],        
                                                language_of_interest =  "nl", 
                                                    from = from,  
                                                    to = to,    
                                                    status = 0) 
        
        wiki.df = cbind(wiki.df,nl.wikipedia.input.data[,-1])
        names(wiki.df)[length(names(wiki.df))] = wiki.pages[i]
      }, error=function(e) if(status==1){ cat("\n Something went wrong with ",wiki.pages[i],": page dropped, continue \n")})
    }
    # add wiki. prefix to identify pages later on
    names(wiki.df)[-1] = paste("wiki.",names(wiki.df)[-1],sep="") 
    
    
    
# Identify relevant GOOGLE DATA: keyword related with 'griep'
    if(status==1){ cat("\n Retrieving Google keyword info")}
    google_primer = gtrends(keyword=term,              
                          geo=country_of_interest,  
                          time=paste(from,to),      
                          gprop ="web")             
      tops = google_primer$related_queries$related_queries=="top" 
      google_related = google_primer$related_queries$value[tops]
      g.trends.keywords = c(term,google_related)
      look.up.g = g.trends.keywords
    
    # Identify additional keyword: related to pages that are related to 'griep'
    for(i in 1:length(look.up.g)){
     if(status==1){ cat(round(i/length(look.up.g),4)*100,"% \n")}
        extended.related = gtrends(keyword=look.up.g[i],            
                                  geo=country_of_interest,  
                                  time=paste(from,to),      
                                   gprop ="web")             
        tops = extended.related$related_queries$related_queries=="top" 
        google_related.extended = extended.related$related_queries$value[tops]
        google_related.extended = google_related.extended[1:5] # only take the top 5 each
        g.trends.keywords = unique(c(g.trends.keywords,google_related.extended)) # many dulicates
      }
    
    # we get rid of everyhting with a year in in. these are no good predictors!
    g.trends.keywords = g.trends.keywords[!grepl("\\d",g.trends.keywords)] 
    if(show.me.plots==1) {g.trends.keywords} # print the keywords for inspection (you could also add keywords manually)

    g.news.keyword = "griep" # set keyword for google news trends
    
    
# Doanload google data
    if(status==1){ cat("\n Downloading Google data")}
  
    # After an unknown amount of queries, Google blocks the API
    # Then you might get the error "res$status_code == 200 is not True..."
    # Try on the next day, or use another IP/Session
  
      g.trends.input =  fGetGoogleData(keyword = g.trends.keywords, 
                                     country_of_interest=country_of_interest,
                                     from=paste(as.Date(from)),
                                     to=paste(as.Date(to)),
                                     status= 1,    
                                     prefix="g.trends.")
    # trends for google news trends
    g.news.input =  fGetGoogleData(keyword = g.news.keyword,
                                   country_of_interest=country_of_interest,
                                   from=paste(as.Date(from)),
                                   to=paste(as.Date(to+1)),
                                   status= 1,    
                                   prefix="g.news.",
                                   gprop="news")    # Retrieving Google News search queries
    
    google.input.data = merge(g.trends.input,g.news.input,by="date",all=T)
    
    # Removing empty columns and those with to little information
    google.input.data = google.input.data[,-(nearZeroVar(google.input.data,uniqueCut = 25))]
```
    
    
    
## 6. Pre-process the data
    
```{r}
# Merging and slicing data sets
    if(status==1){ cat("\n Preprocessing data")}
    if(class(ili.nl.data$date )=="Date"){
      ili.nl.data$date = ISOweek(ili.nl.data$date ) 
      }
    
    
    df.full = merge(ili.nl.data,google.input.data, by="date")
    df.full = merge(df.full,wiki.df, by="date")
    
    
    df.full$date = ISOweek2date(paste(df.full$date,"-7",sep="")) # 
    if(status==1){ cat("\n Full data set:",dim(df.full)[1], "Weeks and",dim(df.full)[2]-2,"Predictors")}
    
    split = which(df.full$date<split.at) 
    
    df.train = df.full[split,-c(1,2)] # Predictor training data set
    y.train = df.full[split,c(2)] # Outcome for training data set
    date.train = df.full[split,c(1)] # Date, not a predictor but useful for plotting
    
    df.test  = df.full[-split,-c(1,2)] # Predictors for testing/evaluation data set
    date.test = df.full[-split,c(1)] # date for test data set
    
    
# NA handling
    sum.NA.train = as.numeric(lapply(df.train,function(x){sum(is.na(x))})) 
    sum.NA.train = sum.NA.train > length(df.train[,1]) * 0.1 
    if(sum(sum.NA.train)>0){
      df.train = df.train[-which(sum.NA.train)]
      df.test = df.test[which(colnames(df.test) %in% colnames(df.train))]}
    # and test data separately
    sum.NA.test = as.numeric(lapply(df.test,function(x){sum(is.na(x))}))
    sum.NA.test = sum.NA.test > length(df.test[,1]) * 0.1 
    if(sum(sum.NA.test)>0){
      df.test = df.test[-which(sum.NA.test)]
      df.train = df.train[which(colnames(df.train) %in% colnames(df.test))]}
    
    # Imputing remaining NAs
    df.train = na.ma(df.train , k = 3, weighting = "exponential") 
    df.test = na.ma(df.test , k = 3, weighting = "exponential") 
    
# Removing features with near zero variance
    nearZeroVar = nearZeroVar(df.train,freqCut = 95/5 , uniqueCut = 25) 
    if(sum(nearZeroVar)>0){
      df.train = df.train[,-nearZeroVar] 
      df.test = df.test[which(colnames(df.test) %in% colnames(df.train))]}
    
    
# Scaling, centering, (no transofrmation, consider boxcox?)
    preprocess.df.train = preProcess(df.train, method=c("scale","center"))
    df.train = predict(preprocess.df.train, newdata = df.train)
    df.test = predict(preprocess.df.train,newdata = df.test)
    if(status==1){ cat("\n\n train data set:",dim(df.train)[1], "Weeks
                       Min =",as.character(date.train[1]),"
                       Max =",as.character(date.train[length(date.train)]))
                   cat("\n\n train data set:",dim(df.test)[1], "Weeks 
                       Min =",as.character(date.test[1]),"
                       Max =",as.character(date.test[length(date.test)]))}

```




## 7. Model building

```{r}
# Define Control Object
controlObject <- trainControl(method = "timeslice",
                              initialWindow = 52,   # First model is trained on 2 years
                              horizon = 1,          # how many weeks used to validate? 
                              fixedWindow = FALSE,  # Origin stays the same
                              allowParallel = TRUE) # Paralel computing can speed things up
    
# paralel computing 
no_cores <- detectCores() - 1  
cl <- makeCluster(no_cores, type="FORK")   # DIFFERENT ON MICROSOFT MACHINES ?!? 
registerDoParallel(cl)  
    
    
# Model training/tuning
# to speed things up, we only use 3 of the 12 models that were used in the original version
# the caret packages offers many other powerful tools for building these models!
    if(status==1){ cat("\n --- Building models ---")}
    
    if(status==1){ cat("\n --- Building PLS ---")}
    # partial least square
    M.pls = train(y= y.train ,
                  x = df.train,
                  method = "pls",
                  tuneLength = 20,
                  trControl = controlObject)
    if(show.me.plots==1) {plot(M.pls)}
    
    
    # lasso regression (glmnet)
    if(status==1){ cat("\n --- Building Lasso ---")}
    # lasso tuning grid
    lassoGrid <- expand.grid(.alpha = c(.2, .4, .6, .8),.lambda = seq(.05, 1, length = 50)) 
    # Model
    M.lasso <- train(y= y.train ,
                     x = df.train,
                     method = "glmnet",
                     family = "gaussian", # tried poisson, worse!
                     tuneGrid = lassoGrid,
                     trControl = controlObject)
    if(show.me.plots==1) {plot(M.lasso)}
    
    # Cubist (cubist) -- Really powerful method, but can take long to finish !!!
    if(status==1){ cat("\n --- Building Cubist ---")}
    # cubist tuning grid
    cubistGrid <- expand.grid(.committees = seq(1,70,by=5),.neighbors=c(3,4,5,6,7,9))
    # Model
    M.cubist = train(y= y.train ,
                     x = df.train,
                     method = "cubist",
                     tuneGrid = cubistGrid,
                     trControl = controlObject)
    if(show.me.plots==1) {plot(M.cubist)}
    
    # Saving results
    models.nl = list(result.list = list(M.pls = M.pls,
                                        M.lasso = M.lasso,
                                        M.cubist = M.cubist), 
                     eval.list = list()) # empty list for storing evaluation results later on

```
    
    
    
## 8. Evaluate models

```{r}
    # Loop for running the model evaluation function
    for(i in 1:length(models.nl$result.list)){
      tryCatch({
        name.of.model = names(models.nl$result.list)[i]
        models.nl$eval.list[[i]] = fEvalModel(models.nl$result.list[[i]])
        names(models.nl$eval.list)[i] = names(models.nl$result.list)[i]},
        error = function(e){cat(names(models.nl$result.list)[i] ,": Error \n")})
    }
    
    # assess lowest CV RMSE per model
    means=NULL; sd = NULL; model.name = NULL
    for(m in 1:length(models.nl$result.list)){
      means[m] = mean(models.nl$result.list[[m]]$resample$RMSE,na.rm=T)
      sd[m] = sd(models.nl$result.list[[m]]$resample$RMSE,na.rm=T)
      model.name[m] = names(models.nl$result.list)[m]
    }
    sd = sd[order(means)]
    model.name = as.character(model.name)
    model.name = model.name[order(means)]
    means = means[order(means)]

# Compare models mean RMSE visually    
model.comparison = 
      ggplot() +
      geom_point(aes(x=means,y=model.name)) +
      geom_line(aes(x=c(means-sd,means+sd),y=rep(model.name,times=2))) +
      ggtitle("Model mean RMSE +/- 1 SD") +
      xlab("RMSE") +
      ylab("Model") 
    
    if(show.me.plots==1) {model.comparison}
```


## 9. Select the winning model

```{r}
# select the model with the lowest mean RMSE 
# (not neccessarily the most reliable metric for a good nowcast model!)
select.model = which(names(models.nl$result.list) == model.name[1])
if(status==1){ cat("\n --- Selected model:",model.name[1],"---")}
final.model  = models.nl$result.list[[select.model]]
```



## 10. Create a forecast
```{r}
# Forecast model using facebook's prophet algorithm
    if(status==1){ cat("\n --- Creating Forecast ---")}
    forecast.date = seq(from=min(date.test), to = forecast.to,by=7)
    null.model = prophet(df=data.frame(ds = date.train,
                                       y=y.train),
                         growth = "linear",
                         yearly.seasonality = T,
                         weekly.seasonality = F)
    forecast = make_future_dataframe(null.model, periods = length(forecast.date),freq="week")
    null.model.forecast = predict(null.model, forecast)
    select.training.preds = ISOweek(null.model.forecast$ds) %in% ISOweek(date.train)
    preds.null.model = null.model.forecast$yhat[select.training.preds]
    forecast = null.model.forecast$yhat[-which(select.training.preds)]
```


## 11. Final results
```{r warning=FALSE}
# Final results (actual versus predicted)
    if(status==1){ cat("\n --- Final Result Plots ---")}

# Training + Nowcast predictions of the final model
preds.train  = predict(final.model)
nowcast      = predict(final.model,newdata=df.test)

#P Plotting
pnf.1 =
  ggplot() +
      # actual outcome
      geom_line(aes(x=date.train,y=y.train,col="black"),size=2) +
      # forecast
      geom_line(aes(x=forecast.date,y=forecast,col="cyan")) +
      geom_point(aes(x=forecast.date,y=forecast,col="cyan")) +
      geom_line(aes(x=date.train,y=preds.null.model,col="cyan")) +
      # nowcast
      geom_line(aes(x=date.train,y=preds.train,col="orange")) +
      geom_line(aes(x=date.test,y=nowcast,col="red")) +
      geom_point(aes(x=date.test,y=nowcast,col="red")) +
      # legend
      scale_color_manual(name ="", 
                         values = c("black" = "black",
                                    "cyan" = "cyan", 
                                    "orange" = "orange",
                                    "red" = "red"),
                         labels = c("Actual incidence",
                                    "Forecast", 
                                    "Nowcast (training)" ,
                                    "Nowcast")) +
      # separating training and test data
      geom_vline(xintercept = as.numeric( as.Date(max(date.train)))) +
      # showing today
      geom_vline(xintercept = as.numeric( Sys.Date()),linetype=2,col="purple") +
      # labels + titles
      ylab("Influenza incidence") +
      xlab("2017/18") +
      ggtitle("Forecast vs Nowcast model: Influenza season 2017/18") +
      theme(plot.title = element_text(size = 10, face = "bold")) +
      # xlim(c(as.Date("2016-11-01"),as.Date("2017-05-01"))) +
      theme_minimal() 
    if(show.me.plots==1) {pnf.1}
    
# Zoom in: Only showing data for the last 1/2 year
pnf.2 = pnf.1 + 
          xlim(min(date.test)-180,forecast.to)
    if(show.me.plots==1) {pnf.2}
    
# both plots combined
combined.plot = plot_grid(pnf.1,pnf.2,nrow=2)
    if(show.me.plots==1) {combined.plot}
  

# Saving numeric nowcast and forecast results in tables
    nowcast.df = data.frame(date=date.test,prediction= nowcast)
    forecast.df = data.frame(date=forecast.date,prediction= forecast)
    nowcast.df$date = nowcast.df$date  +1
    forecast.df$date = forecast.df$date+1
    write.csv(nowcast.df,paste("~/nowcast",Sys.Date(),".csv"))
    write.csv(forecast.df,paste("~/forecast",Sys.Date(),".csv"))
```


## 12. Savegame

```{r}
# Saves the entire sesssion (all objects in the global envir) as an .rdata file
    file.name = paste("nft_data_",Sys.Date(),".rdata",sep="")
    save(list = ls(environment()), file = file.name)
```
   
   
   
## 13. Finish
```{r}
    time2 = Sys.time()
    cat(" \n All Data has been stored in", file.name)
    cat("\n Time elapsed: ",time2-time1)
    cat("\n --- Done! ---")
```
    
    
## 14. References

```{r}
# R-Packages used for this document
citations <- function() {
  citation.list = list()
  citation.list[[1]] =  RStudio.Version()$citation
  cit.list <- c(required_packages[order(required_packages)])
  for(i in 1:length(cit.list)) {
    ref <- citation(cit.list[i])
    citation.list[[i+1]] = ref
    }
return(citation.list)
}
citation.list =  citations() 
citation.headers = c("R Studio",required_packages[order(required_packages)])
for(citation in 1:length(citation.list)){
  cat("'",citation.headers[citation],"' \n",sep="")
print(citation.list[[citation]],style="text")
cat("\n")
  }

```
    
    
    
    
    
